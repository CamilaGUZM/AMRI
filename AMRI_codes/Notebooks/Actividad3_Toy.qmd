---
title: "Activad3_Toy"
format:
   html:
     mathjax: true
     toc: true
     html-math-method: katex
     embed-resources: true
     self-contained-math: true
     df-print: kable
editor: source
---

# Objetivo

Construye un clasificador naïve Bayes para clasificar la observación $\tilde{x}=(0.4,1.5,\text{Level1})$

`{r} #| eval: false} install.packages("tidyverse")`

```{r}
library(tidyverse)
```

## Cargar datos

```{r}
data = read.csv("../data/toy_bayes.csv", stringsAsFactors = TRUE)
head(data)
```

![](images/clipboard-3107166103.png)

# Hacer la observación nueva

```{r}
x_tilde = tibble(y = c("Class0", "Class1"),
                 x1 = rep(0.4, 2), x2 = rep(1.5, 2))
```

# Prior

$$
\mathbb{P}(Y = k)
$$

-   Iniciaremos la estimación de nuestra red bayesiana con el nodo padre, es decir, la distribución prior de la variable de clase $y$.

-   Para esto simplemente contamos el número de observaciones en cada clase y dividimos entre el total.

```{r}
prior = data |>
            group_by(y) |>  ## Agrupamos por la etiqueta
            summarise(n = n()) |> ## Contamos el número de observaciones de cada clase
            mutate(prior_prob = n/sum(n)) ## Calculamos las probabilidades a priori
prior
```

-   funcion n() solo cuenta (como COUNT en excel)
-   mutate crea nuevas variables o modifica alguna que ya existe

# Visualizaciones

## Función de densidad para x1 y Class0

```{r}
data |>
    filter(y == "Class0") |>
    ggplot(aes(x = x1, y = after_stat(density))) +
    geom_histogram(color = "dodgerblue", fill = "slategray1", alpha = 0.4) +
    geom_density(fill = "dodgerblue", color = NA, lwd = 1, alpha = 0.5) +
    labs(x = expression(x[1]), y = "Density", title = expression(x[1] ~ "|" ~ y == 0)) +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold",
                                    margin = margin(0, 0, 5, 0)),
            axis.title.x = element_text(face = "bold"),
            axis.title.y = element_text(face = "bold", angle = 90),
            legend.title = element_text(hjust = 0.5, face = "bold"),
            legend.text = element_text(hjust = 0.5),
            strip.text = element_text(size = 14, hjust = 0.5, face = "bold",
                                      margin = margin(2, 3, 3, 3)))
```

## Bar de $x_3$ en clase 0

-   Realizamos un bar chart para visualizar la distribución de $x_3$ en la clase 0.

```{r}
data |>
    filter(y == "Class0") |>
    ggplot(aes(x = x3, y = after_stat(count/sum(count)))) +
    geom_bar(color = "tomato1", fill = "tomato1", alpha = 0.4) +
    labs(x = expression(x[3]), y = "Probability", title = expression(x[3] ~ "|" ~ y == 0)) +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold",
                                    margin = margin(0, 0, 5, 0)),
            axis.title.x = element_text(face = "bold"),
            axis.title.y = element_text(face = "bold", angle = 90),
            legend.title = element_text(hjust = 0.5, face = "bold"),
            legend.text = element_text(hjust = 0.5),
            strip.text = element_text(size = 14, hjust = 0.5, face = "bold",
                                      margin = margin(2, 3, 3, 3)))
```

\|\> significa pipe, $y$ es la densidad,

(y= 0 ) es la categoría

![](images/clipboard-66269458.png)

------------------------------------------------------------------------

# Estimar densidad de $f(x_1 \mid y) \text{ y } f(x_2 \mid y)$

-   lo haremos de forma no paramétrica

    -   tipo kernel

    -   (la curva fea de atrás del gráfico)

$n$ = num observaciones

$h$ = factor the bandwidth, hiperparámetro de suavizamiento

por la suma de todas las observaciones de la función K() \<-- kernel function

$$
\hat{f}_1(x_1 \mid y) = 
\frac{1}{n h} \sum_{i=1}^{n} K\!\left(\frac{x_1 - x_i}{h}\right)
$$

-   todas las formas de estimar el kernel SON CON ESTA FÓRMULA

# Calcular Bandwiths

vamos a extraer bandwiths

-   La estimación de las densidades se realiza mediante la función `density()`.

-   Podemos acceder a los bandwidths mediante el operador `$`.

-   Para cada clase, obtenemos los bandwidths de $x_1$ y $x_2$.

```{r}
bws = data |>
        group_by(y) |>                            ## Agrupamos por la etiqueta
        summarise(bw1 = density(x1)$bw,  ## Extraemos los bandwidths
                   bw2 = density(x2)$bw)
bws
```

extraemos bw para cada clase para cada x_i

tienen que tener repetir cada observacion por clase (si hubieran 3 se hace tres veces)

--

# Función k

-   Implementamos una función que calcule el kernel gaussiano.

```{r}
K = function(x){
    return(exp(-x^2/2)/ sqrt(2*pi))
}
```

-   usar estimador tipo kernel

-    implementamos una función para calcular la densidad estimada usando ese kernel.

```{r}
kernel = function(x, data, bw){
    return(mean(K((x-data)/bw))/bw)
}


```

-   Con esto tendríamos totalmente estimadas $f(x_1 \mid y) \text{ y } f(x_2 \mid y)$

-   Para estimar $f(x_3 \mid y)$ basta con contar el número de observaciones en cada nivel de esta variable en cada clase y dividirlo entre el total.

## Ejemplo con x_3

ver número de veces que sale cada categoría de $x_3$ y la probabilidad de que salga cada una

-   Por ejemplo, para la primera clase estimamos la probabilidad de cada uno de los niveles de $x_3$

```{r}
data |>
    filter(y == "Class0") |>
    group_by(x3) |>
    summarise(n = n()) |>
    mutate(prob = n/sum(n))
```

group es solo decirle que está agrupado

sumarize le dice que empieze a hacer conteos

n = numero de observaciones

prob de cada Level

# Calcular nueva Observación

Para realizar la clasificación de la nueva observación usando nuestro **clasificador naïve Bayes** usaremos la **regla de Bayes**:

$$
\mathbb{P}(Y = k \mid \mathbf{X} = \tilde{\mathbf{x}}) =
\frac{\mathbb{P}(Y = k) \displaystyle\prod_{j=1}^{3} \hat{f}^{j}(\tilde{x}_{j} \mid y = k)}
{\displaystyle \sum_{\ell = 0}^{1} \mathbb{P}(Y = \ell) \prod_{j=1}^{3} \hat{f}^{j}(\tilde{x}_{j} \mid y = \ell)}
$$

Necesitamos evaluar $\hat{f}^{1}(x_{1} \mid y) \quad \text{y} \quad \hat{f}^{2}(x_{2} \mid y)$ en $\tilde{x}_{1} \quad \text{y} \quad \tilde{x}_{2}$ respectivamente.

## Armar Tabla con observación nueva

-   Creamos una tabla uniendo las nuevas observaciones con la tabla donde encontramos los bandwidths de $x_1$​ y $x_2$​ para cada clase usando la función `left_join()`.

```{r}
num_probs = x_tilde |>
                left_join(bws, by = "y")
num_probs
```

-   conviene tenerlo así por el teorema de bayes

-   \[agregar de foto\]

-   conviene porque puedes sacar x arruga mid y y x arruga 2 mid y y la suma

-   la col x1 es x_1 arruga mid y

-   col x2 x_2 arruga mid y

-   lor renglones son k =1 y k= 2

---

# Density Kernel Evaluado

Para cada clase, evaluamos el density kernel estimator en la nueva observación y seleccionamos solamente la clase y el resultado de $\hat{f}_1(\tilde{x}_1 \mid y)$ y $\hat{f}_2(\tilde{x}_2 \mid y)$

```{r}
num_probs = num_probs |>
                group_by(y) |>                            ## Agrupamos por la etiqueta
                summarise(kernel1 = kernel(x1, data$x1,bw1),   ## Calculamos los kernels
                           kernel2 = kernel(x2, data$x2, bw2)) |>
                select(y, kernel1, kernel2) ## Seleccionamos sólo la etiqueta y los kernels
num_probs
```

calculamos $f_3(\tilde{x}_3 \mid y)$ (parted de arriba de bayes), en x3 agarra solo el valor de Level 1

-   Para obtener $f_3(\tilde{x}_3 \mid y)= \mathbb{P}(X_3=Level1 \mid y)$ , simplemente contamos el número de observaciones donde $x_3= \text{Level1}$ para cada clase, y dividimos entre el total de observaciones en cada clase.

```{r}
cat_probs = data |>
                group_by(y, x3) |> ## da 6 combinaciones   ## Realizamos la agrupación por la etiqueta y x3
                summarise(n = n()) |>    ## Contamos el número de observaciones, estas son las combinaciones, se divide entre cada clase de y
                group_by(y) |>        ## Agrupamos por la etiqueta
                mutate(prob = n/sum(n)) |> ## Calculamos las probabilidades
                filter(x3 == "Level1")           ## Seleccionamos sólo el caso cuando x3 = Level1
```

```{r}
cat_probs
```

-   Calculamos el denominador de la regla de Bayes sumando $\mathbb{P}(Y=k)f_1(\tilde{x}_1 \mid y)f_2(\tilde{x}_2 \mid y)f_3(\tilde{x}_3 \mid y)$  para cada clase.

```{r}
total = sum(prior$prior_prob * num_probs$kernel1 * num_probs$kernel2 * cat_probs$prob)
```


-   Finalmente, obtenemos las probabilidades a posteriori para cada clase.

```{r}
(prior$prior_prob * num_probs$kernel1 * num_probs$kernel2 * cat_probs$prob)/total
```

resultado es \[class 0, Class 1\]

-   Asignaríamos la nueva observación a la segunda clase (`Class1`).
