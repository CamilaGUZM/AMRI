---
title: "Activad3_Toy"
format:
   html:
     mathjax: true
     toc: true
     html-math-method: katex
     embed-resources: true
     self-contained-math: true
     df-print: kable
editor: source
---

```{r} #| eval: false}
install.packages("tidyverse")
```

```{r}
library(tidyverse)
```

```{r}
data = read.csv("../data/toy_bayes.csv", stringsAsFactors = TRUE)
head(data)
```

Con func dencidad

```{r}
data |>
    filter(y == "Class0") |>
    ggplot(aes(x = x1, y = after_stat(density))) +
    geom_histogram(color = "dodgerblue", fill = "slategray1", alpha = 0.4) +
    geom_density(fill = "dodgerblue", color = NA, lwd = 1, alpha = 0.5) +
    labs(x = expression(x[1]), y = "Density", title = expression(x[1] ~ "|" ~ y == 0)) +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold",
                                    margin = margin(0, 0, 5, 0)),
            axis.title.x = element_text(face = "bold"),
            axis.title.y = element_text(face = "bold", angle = 90),
            legend.title = element_text(hjust = 0.5, face = "bold"),
            legend.text = element_text(hjust = 0.5),
            strip.text = element_text(size = 14, hjust = 0.5, face = "bold",
                                      margin = margin(2, 3, 3, 3)))
```

\|\> significa pipe, \$y\$ es la densidad,

(y= 0 ) es la categoría lol

## Estimar densidad de f(x_1 \\mid y) y f(x_2 \\mid y) 

-   lo haremos de forma no paramétrica

    -   tipo kernel

    -   (la curva fea de atrás del gráfico)

n = num observaciones

h = factor the bandwidth, hiperparámetro de suavizamiento

por la suma de todas las observaciones de la función K(ver presentación) \<-- kernel function

-   todas las formas de estimar el kernel SON CON ESTA FÓRMULA

```{r}
data |>
    filter(y == "Class0") |>
    group_by(x3) |>
    summarise(n = n()) |>
    mutate(prob = n/sum(n))
```

group es solo decirle que está agrupado

sumarize le dice que empieze a hacer conteos

n == numero de observaciones

prob de cada 1

**Falta doña prior**

$$
\mathbb{P}(Y = k)
$$

se calcula igual que fórmula anterior

```{r}
prior = data |>
            group_by(y) |>                        ## Agrupamos por la etiqueta
            summarise(n = n()) |>                        ## Contamos el número de observaciones de cada clase
            mutate(prior_prob = n/sum(n))  ## Calculamos las probabilidades a priori
```

-   funcion n() solo cuenta (como COUNT en excel)

```{r}
prior
```

mutate crea nuevas variables o modifica alguna que ya existe

vamos a extraer bandwiths

```{r}
bws = data |>
        group_by(y) |>                            ## Agrupamos por la etiqueta
        summarise(bw1 = density(x1)$bw,  ## Extraemos los bandwidths
                   bw2 = density(x2)$bw)
bws
```

extraemos bw para cada clase para cada x_i

```{r}
x_tilde = tibble(y = c("Class0", "Class1"),
                 x1 = rep(0.4, 2), x2 = rep(1.5, 2))
x_tilde
```

tienen que tener repetir cada observacion por clase (si hubieran 3 se hace tres veces)

```{r}
num_probs = x_tilde |>
                left_join(bws, by = "y")
num_probs
```

-   conviene tenerlo así por el teorema de bayes

-   \[agregar de foto\]

-   conviene porque puedes sacar x arruga mid y y x arruga 2 mid y y la suma

-   la col x1 es x_1 arruga mid y

-   col x2 x_2 arruga mid y

-   lor renglones son k =1 y k= 2

--

función k

```{r}
K = function(x){
    return(exp(-x^2/2)/ sqrt(2*pi))
}
```

usar estimador tipo kernel

```{r}
kernel = function(x, data, bw){
    return(mean(K((x-data)/bw))/bw)
}


```

```{r}
num_probs = num_probs |>
                group_by(y) |>                            ## Agrupamos por la etiqueta
                summarise(kernel1 = kernel(x1, data$x1,bw1),   ## Calculamos los kernels
                           kernel2 = kernel(x2, data$x2, bw2)) |>
                select(y, kernel1, kernel2) ## Seleccionamos sólo la etiqueta y los kernels
num_probs
```

calculamos f(x_3 \mid y (parted e arriba de bayes), en x3 agarra solo el valor de Level 1

```{r}
cat_probs = data |>
                group_by(y, x3) |> ## da 6 combinaciones   ## Realizamos la agrupación por la etiqueta y x3
                summarise(n = n()) |>    ## Contamos el número de observaciones, estas son las combinaciones, se divide entre cada clase de y
                group_by(y) |>        ## Agrupamos por la etiqueta
                mutate(prob = n/sum(n)) |> ## Calculamos las probabilidades
                filter(x3 == "Level1")           ## Seleccionamos sólo el caso cuando x3 = Level1
```

```{r}
cat_probs
```

```{r}
total = sum(prior$prior_prob * num_probs$kernel1 * num_probs$kernel2 * cat_probs$prob)
```

la posterior

```{r}
(prior$prior_prob * num_probs$kernel1 * num_probs$kernel2 * cat_probs$prob)/total
```

resultado es \[class 0, Class 1\]
